{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA5AjxM8-c3C"
      },
      "source": [
        "# SPARK ML (Machine Learning with Spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0EYxkCS-sju",
        "outputId": "f6c8a0e7-3d73-4842-d253-7fbc3a258ceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=782360d7bb39368b483681c9e1321c4b455124cb750bc492193e0a7f39932b96\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVs8a8R_KcSo"
      },
      "source": [
        "### Creating Session and Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcJf9gH6-c3G"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPEf7ecrp1VN",
        "outputId": "0abf3c78-d82f-4c68-abde-bc4483a163e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  1.0|(20,[0,3,19],[3.0...|\n",
            "|  0.0|(20,[0,4,11],[3.0...|\n",
            "|  1.0|(20,[1,2,14],[1.0...|\n",
            "+-----+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "libsvm = '''\n",
        "1 1:3.0 4:5.0 20:1.0\n",
        "0 1:3.0 5:5.0 12:1.0\n",
        "1 2:1.0 3:2.0 15:1.0\n",
        "'''\n",
        "\n",
        "open(\"sample_libsvm.txt\",\"w\").write(libsvm)\n",
        "\n",
        "df_toy = spark.read.format('libsvm').load('./sample_libsvm.txt')\n",
        "\n",
        "df_toy.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-fb8CKz-c3H"
      },
      "source": [
        "### Creating a Data Frame as running example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcvNhtU9LJUY",
        "outputId": "f01a8ac8-2152-4e04-c077-7a715e580fe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- reviewer: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- label: long (nullable = true)\n",
            "\n",
            "+--------+--------------------+-----+\n",
            "|reviewer|                text|label|\n",
            "+--------+--------------------+-----+\n",
            "|     Joe|The movie was dis...|    0|\n",
            "|     Mae|I liked very much...|    1|\n",
            "|     Joe|What ugly end, di...|    0|\n",
            "|    Mick|This film makes m...|    1|\n",
            "|     Mae| Fantastic argument!|    1|\n",
            "+--------+--------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [(\"Joe\",\"The movie was disapointing\", 0),\n",
        "        (\"Mae\",\"I liked very much the film\", 1),\n",
        "        (\"Joe\",\"What ugly end, didn't like at all \", 0),\n",
        "        (\"Mick\",\"This film makes me happy\", 1),\n",
        "        (\"Mae\",\"Fantastic argument!\",1)]\n",
        "\n",
        "columns = [\"reviewer\",\"text\",\"label\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVFcMN9PBw2M"
      },
      "source": [
        "Splitting the data into train/test partitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la-TRXDG-c3H"
      },
      "outputs": [],
      "source": [
        "train, test = df.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9JYVsFNuC9C",
        "outputId": "403fa48a-5fed-4629-fa7b-e1d87f45335c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hli2QSfDr9P9"
      },
      "source": [
        "### Mounting the Pipeline, learn and predict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncXVAVd8r1ZB"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3my13R4EvnG"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20) #for real text should be 1000-2000\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "nb = NaiveBayes(modelType=\"multinomial\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw44aTHnoZMZ"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(stages=[tokenizer,hashingTF,idf,nb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhZ9Ox3-x8F3",
        "outputId": "03bacda3-4bdd-4c65-a715-8435848bf665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "featuresCol: features column name. (default: features)\n",
            "labelCol: label column name. (default: label)\n",
            "modelType: The model type which is a string (case-sensitive). Supported options: multinomial (default), bernoulli and gaussian. (default: multinomial, current: multinomial)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "smoothing: The smoothing parameter, should be >= 0, default is 1.0 (default: 1.0)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
          ]
        }
      ],
      "source": [
        "print(nb.explainParams())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCSPo8fKyDup",
        "outputId": "9df93fe3-bb20-4ed4-8482-8ccd539eaab9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Param(parent='NaiveBayes_8e5e6c1832a5', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0')"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nb.smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfhGtwxnt7KY"
      },
      "source": [
        "Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeS_2rjDuZ5T"
      },
      "outputs": [],
      "source": [
        "model = pipeline.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ_zENbXt-SB"
      },
      "source": [
        "Predictions with the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2HI_OPp09yG"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9hIYN_TteXZ",
        "outputId": "eea447e3-907b-4213-c174-e9be9000195b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+--------------------+\n",
            "|label|prediction|         probability|\n",
            "+-----+----------+--------------------+\n",
            "|    0|       0.0|[0.69456313923456...|\n",
            "+-----+----------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.select(\"label\",\"prediction\",\"probability\").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLIA2iEPt4Ps"
      },
      "source": [
        "### Evaluating the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaLKudKLtvSZ"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb3Brsb8uTU7"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RelACCNVuZIm",
        "outputId": "f904a69e-214c-463a-8d84-11174e84380e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbyupVD3WLUQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dblm80jlWMKE"
      },
      "source": [
        "We want to create a learning model for predicting the 5-star rates of the movies dataset used in the previous deliverable. Split the dataset into two partitions: train (80%) and test (20%). Choose the model you think is best suited for this task (e.g., Naive Bayes, Logistic Regression, etc.) and perform the dataset transformations you consider relevant to obtain the feature vectors for training the chosen model. Finally, evaluate the performance of the model with the appropriate evaluator over the predictions in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55BM0PkjAeS2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count\n",
        "from google.colab import drive\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from xgboost.spark import SparkXGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m13RkZPWirV2",
        "outputId": "a17ada29-2e02-4b15-f60e-349d2ef6c849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR3Aajn0An3-"
      },
      "outputs": [],
      "source": [
        "# Init spark session\n",
        "spark = SparkSession.builder.appName(\"films\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "file_path = \"/content/gdrive/MyDrive/Master_23_24/Big_Data/filmsML.txt/part-00000-145dc166-9f44-4c07-abdc-289b9a8cd6dc-c000.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I have some modifications in notebook 6. I have reduced the number of features (delate stop-words, words that just appears one time and words with a tf*idf below the 25th percentile of the total words of the document)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWcabxo2o2Uf",
        "outputId": "83e31c88-bd2a-48cd-b2cb-d67a7379bab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  4.0|(40018,[0,1,2,3,5...|\n",
            "|  4.0|(40018,[7,33,39,1...|\n",
            "|  4.0|(40018,[43,47,133...|\n",
            "|  3.0|(40018,[2,28,81,8...|\n",
            "|  3.0|(40018,[2,35,43,6...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read the preprocesed dataset\n",
        "df = spark.read.format('libsvm').load(file_path)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDax3T7iauK3",
        "outputId": "33a07821-c0a3-458c-9a47-d036e995e75a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+------------------+\n",
            "|label|count|        percentage|\n",
            "+-----+-----+------------------+\n",
            "|  1.0|  351| 9.051057246003094|\n",
            "|  4.0|  890| 22.94997421351212|\n",
            "|  3.0| 1253| 32.31046931407942|\n",
            "|  2.0|  923|23.800928313563695|\n",
            "|  5.0|  461|11.887570912841671|\n",
            "+-----+-----+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calcula el total de películas en el DataFrame\n",
        "total_movies = df.count()\n",
        "\n",
        "# Agrupa por la columna 'label' y cuenta las ocurrencias\n",
        "count_by_label = df.groupBy('label').agg(count('*').alias('count'))\n",
        "\n",
        "# Calcula el porcentaje sobre el total para cada clase\n",
        "count_by_label_with_percentage = count_by_label.withColumn('percentage', (col('count') / total_movies) * 100)\n",
        "\n",
        "count_by_label_with_percentage.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8qrnJJRjuX6"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXGAZYlDgFF1"
      },
      "outputs": [],
      "source": [
        "# Dividir los datos en conjuntos de entrenamiento y prueba (80% para entrenamiento y 20% para prueba)\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Crear el clasificador RandomForest\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=5)\n",
        "\n",
        "# Construir el pipeline\n",
        "pipeline = Pipeline(stages=[rf])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model = pipeline.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cy2wi_whXQq"
      },
      "outputs": [],
      "source": [
        "# Realizar predicciones en el conjunto de prueba\n",
        "predictions = model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvvTVirahSHa",
        "outputId": "879f0fb9-1cba-4687-f517-f2812aa808b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 30.64%\n"
          ]
        }
      ],
      "source": [
        "# Evaluación del modelo\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy = {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mysNSPQfgmMv",
        "outputId": "0f6416fc-ae93-490f-e157-63c1dd43a307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|  1.0|       3.0|\n",
            "|  1.0|       3.0|\n",
            "|  1.0|       3.0|\n",
            "|  1.0|       3.0|\n",
            "|  1.0|       2.0|\n",
            "+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.select(\"label\",\"prediction\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOhB-s6Ia5s0"
      },
      "source": [
        "Logistic Regresion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRBBzwMnt7YP"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Crear el clasificador RandomForest\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=3)\n",
        "\n",
        "# Construir el pipeline\n",
        "pipeline = Pipeline(stages=[lr])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model = pipeline.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ2f8ofucSme"
      },
      "outputs": [],
      "source": [
        "# Realizar predicciones en el conjunto de prueba\n",
        "predictions = model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLb5Z5wBcTIr",
        "outputId": "f2d3544d-9107-40e2-c1aa-46d1be013058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 38.02%\n"
          ]
        }
      ],
      "source": [
        "# Evaluación del modelo\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy = {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQBxA5KKcVNX",
        "outputId": "1a35a439-5652-47e5-a5a1-80ece74cac1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|  1.0|       2.0|\n",
            "|  1.0|       3.0|\n",
            "|  1.0|       5.0|\n",
            "|  1.0|       3.0|\n",
            "|  1.0|       2.0|\n",
            "+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.select(\"label\",\"prediction\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iALFIlMkCUT"
      },
      "source": [
        "XGBoost, I can't doing that it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M66qiIiilUWb"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "xgb = SparkXGBClassifier(features_col=\"features\",\n",
        "                         label_col=\"label\",\n",
        "                         prediction_col=\"prediction\")\n",
        "\n",
        "\n",
        "model2 = xgb.fit(train_data)\n",
        "\n",
        "predictions2 = xgb.predict(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eiEbno2jVuU"
      },
      "outputs": [],
      "source": [
        "# Evaluación del modelo\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions2)\n",
        "print(\"Accuracy = {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tjp_cVuJjWLQ"
      },
      "outputs": [],
      "source": [
        "predictions2.select(\"label\",\"prediction\").show(721)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG8pHM0Au2wB"
      },
      "source": [
        "\n",
        "I going to try the same algoritms but, I going to preproces the original data in a pysparkML pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7BIiSccvcy7",
        "outputId": "4b4f3528-1bfd-4f75-dccd-5a6b08b66090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-02 14:48:10--  http://krono.act.uji.es/IDIA/criticas_pelis.csv.gz\n",
            "Resolving krono.act.uji.es (krono.act.uji.es)... 150.128.97.37\n",
            "Connecting to krono.act.uji.es (krono.act.uji.es)|150.128.97.37|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://krono.act.uji.es/IDIA/criticas_pelis.csv.gz [following]\n",
            "--2023-11-02 14:48:10--  https://krono.act.uji.es/IDIA/criticas_pelis.csv.gz\n",
            "Connecting to krono.act.uji.es (krono.act.uji.es)|150.128.97.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4447654 (4.2M) [application/x-gzip]\n",
            "Saving to: ‘criticas_pelis.csv.gz.1’\n",
            "\n",
            "criticas_pelis.csv. 100%[===================>]   4.24M  3.99MB/s    in 1.1s    \n",
            "\n",
            "2023-11-02 14:48:12 (3.99 MB/s) - ‘criticas_pelis.csv.gz.1’ saved [4447654/4447654]\n",
            "\n",
            "gzip: criticas_pelis.csv already exists; do you wish to overwrite (y or n)? ^C\n"
          ]
        }
      ],
      "source": [
        "!wget \"http://krono.act.uji.es/IDIA/criticas_pelis.csv.gz\"\n",
        "!gunzip \"criticas_pelis.csv.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN0wD-dnvXkn",
        "outputId": "e3d526ce-cc40-4a3e-f969-60b4e4affd0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---------+--------------------+---+\n",
            "| _c0|      _c1|                 _c2|_c3|\n",
            "+----+---------+--------------------+---+\n",
            "|Row0|   File-0| May, ¿quieres se...|  4|\n",
            "|Row1|   File-1| Cómo ponerse en ...|  4|\n",
            "|Row2|  File-10| Deliciosa comedi...|  4|\n",
            "|Row3| File-100| La ironía es el ...|  3|\n",
            "|Row4|File-1000| Al final, y teni...|  3|\n",
            "+----+---------+--------------------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"films2\").getOrCreate()\n",
        "\n",
        "# Path to CSV file\n",
        "csv = \"criticas_pelis.csv\"\n",
        "\n",
        "# Load compressed CSV file into Spark DataFrame\n",
        "df = spark.read.options(header=False).csv(csv)\n",
        "\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eCNRTZ_viPI",
        "outputId": "30bbb664-418c-4f39-ba94-05e81345095a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|                text|label|\n",
            "+--------------------+-----+\n",
            "| May, ¿quieres se...|    4|\n",
            "| Cómo ponerse en ...|    4|\n",
            "| Deliciosa comedi...|    4|\n",
            "| La ironía es el ...|    3|\n",
            "| Al final, y teni...|    3|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cambiar el nombre de la columna \"Name\" a \"Full Name\"\n",
        "# y el nombre de la columna \"City\" a \"Location\"\n",
        "df = df.withColumnRenamed(\"_c2\", \"text\").withColumnRenamed(\"_c3\", \"label\")\n",
        "\n",
        "# Eliminar las columnas \"Age\" y \"Salary\"\n",
        "df = df.drop(\"_c0\", \"_c1\")\n",
        "\n",
        "# Mostrar el DataFrame después de eliminar las columnas\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kfoapyj5xr4C",
        "outputId": "14666019-2dd8-457f-f28d-4c25a5b61c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|                text|label|\n",
            "+--------------------+-----+\n",
            "| May, ¿quieres se...|    4|\n",
            "| Cómo ponerse en ...|    4|\n",
            "| Deliciosa comedi...|    4|\n",
            "| La ironía es el ...|    3|\n",
            "| Al final, y teni...|    3|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cambiar una columna de tipo string a tipo numérico\n",
        "df = df.withColumn(\"label\", col(\"label\").cast(\"int\"))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsB2gPlnu3lC"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2000) #for real text should be 1000-2000\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=5)\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer,hashingTF,idf,rf])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyHHFpBawy3X"
      },
      "outputs": [],
      "source": [
        "# Dividir los datos en conjuntos de entrenamiento y prueba (80% para entrenamiento y 20% para prueba)\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model = pipeline.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flP2UgrWxJ5n"
      },
      "outputs": [],
      "source": [
        "# Realizar predicciones en el conjunto de prueba\n",
        "predictions = model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kilHn2vxxPOB",
        "outputId": "dd6d83ea-bdab-47cf-f24a-70612963e01f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 31.81%\n"
          ]
        }
      ],
      "source": [
        "# Evaluación del modelo\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy = {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grVHm1tKxXFJ",
        "outputId": "327ee7ac-40ec-4809-a37a-bb0006e11e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|    4|       3.0|\n",
            "|    3|       3.0|\n",
            "|    4|       3.0|\n",
            "|    2|       3.0|\n",
            "|    4|       3.0|\n",
            "+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.select(\"label\",\"prediction\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrGuCNrud4Aa"
      },
      "source": [
        "Logisic Regresion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFw2Biszd6sG"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000) #for real text should be 1000-2000\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=6)\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer,hashingTF,idf,lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r0priu2eMmB"
      },
      "outputs": [],
      "source": [
        "# Dividir los datos en conjuntos de entrenamiento y prueba (80% para entrenamiento y 20% para prueba)\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model = pipeline.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pHNxbjAeNJZ"
      },
      "outputs": [],
      "source": [
        "# Realizar predicciones en el conjunto de prueba\n",
        "predictions = model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DDqthineOyI",
        "outputId": "6392bbcd-9a44-4176-cfc7-f1f26d313d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy = 41.25%\n"
          ]
        }
      ],
      "source": [
        "# Evaluación del modelo\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy = {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJLZnPjdeQzx",
        "outputId": "5ed8de29-daed-4995-8b38-1ad47d0fdc8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|    4|       4.0|\n",
            "|    3|       3.0|\n",
            "|    4|       3.0|\n",
            "|    2|       3.0|\n",
            "|    4|       2.0|\n",
            "+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.select(\"label\",\"prediction\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNkbLYZsAJSn"
      },
      "source": [
        "After testing several algorithms, first with the preprocessed data from notebook 6 and then implementing a pipeline that takes the raw data and processes it until obtaining the appropriate data set to apply the machine learning algorithms. In both cases, Random Forest and Logistic Regression have been used, testing with different hyperparameters, a maximum precision of 41.25% has been obtained using Logistic Regression. This very low performance of the model is mainly due to the fact that we are using as characteristics the TF*IDF of the words that form the reviews of the movies, this being a not very powerful metric to perform tasks of this type, compared to current techniques of natural language processing that uses embeddings (dense vectors in which the text is encoded) that are capable of capturing the semantics of different text sequences, which allows obtaining a richer and more meaningful representation of the textual content. Embeddings are capable of capturing semantic and contextual relationships between words, resulting in better understanding of the data by the machine learning model.\n",
        "\n",
        "In contrast, TF*IDF simply assigns a weight to each word based on its frequency in a document and its inverse frequency in the data set. Although this technique can be useful in certain contexts, such as information retrieval or document classification, it is not sufficient to capture the semantic complexity of movie reviews.\n",
        "\n",
        "To improve model performance, one could consider adopting more advanced natural language processing techniques, such as using pre-trained language models such as BERT, GPT, or FastText. These models are capable of generating contextualized embeddings, which means that the representation of a word can vary depending on the context in which it appears in the sentence. This provides a deeper understanding of the structure and meaning of the text."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "name": "quickstart",
    "notebookId": 1927513300154480
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
